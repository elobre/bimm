# A Physical Model for Microstructural Characterization and Segmentation for 3D Tomography Data

This repository contains the code for our [paper](http://arxiv.org/abs/2009.07218) [1], including a Pytorch implementation of the Blurred Interface Mixture Model (BIMM) for analysis of 3D tomography data.

Examples are provided of how to utilize the BIMM for direct quantification and segmentation on artificial and experimental data.

* **Direct quantification**: Determine material parameters directly from the raw image data, without segmentation; volume fractions, interface areas, phase intensities, image resolution and noise levels.

* **Segmentation**: Perform a maximum probability segmentation on the data based on the fitted model.

If you run into any problems running this code, or if you have any questions about the method, feel free to contact us.

## Contents

* `/bimmquant/`: Code for the **bimmquant** library. See description below.
* `/examples/`: Runnable notebooks with examples of how to use the BIMM for structural quantification and segmentation.

    * `/exp_data_quantification_and_segmentation.ipynb`:

      Quantification and segmentation of experimental data.

    * `/art_data_quantification_and_segmentation.ipynb`:

      Quantification on artificial data, comparison with ground truth. Segmentation.

    * `/art_data_generation.ipynb`:

      Code for generating an artificial dataset used in the example notebook on artificial data.

    * `/example_log/`: Directory where tensorboard logs will be saved after model fitting.

### The `bimmquant` library

* `/models/`: The different models considered in the paper: The 1D and 2D versions of the BIMM
* `/data/`: code for importing data for 1D (intensity) or 2D (intensity-gradient) model fitting (`bimm1d.py`, `bimm2d.py`), the Gaussian mixture model (GMM) `gmm.py`, and the Partial volume mixture model (PVMM) `pvmm.py`.
* `/utils/`: `tools.py` for plotting, logging, segmentation etc.

### Image data

3D image data for testing the code is not included in this repo. It must be *generated* or *downloaded* as described in the example notebooks:

- The artificial data used in the notebook `art_data_quantification_and_segmentation.ipynb` is generated by the notebook `art_data_generation.ipynb`.
- The experimental data used in the notebook `exp_data_quantification_and_segmentation.ipynb` can be downloaded from *Zenodo* as described in the notebook intro.


### Installation
In the folder containing `setup.py`, run

    pip install .


This code was tested on Ubuntu 18.04.4 LTS using Pytorch version 1.3.1.


### Reference

If you find this code useful in your research, please consider citing:

[1] Brenne, E. O., Dahl, V. A., & JÃ¸rgensen, P. S. (2020). *A Physical Model for Microstructural Characterization and Segmentation of 3D Tomography Data*. [arXiv:2009.07218](https://arxiv.org/abs/2009.07218)


## UPDATE: Huge speed-up for BIMM2D code

Since the initial release for the code and the paper, an improved implementation of the BIMM2D has lead to a huge speed-up of the BIMM2D model fitting: What previously took 4 min 2 sec, now finished in 15 sec (2 phases, batch size 50, 1000 MC samples, 2000 iterations).

In particular, the speed-up is due to an improved implementation of the modified Bessel function of first kind. Earlier, a general `scipy`-implementation was used ([`scipy.special.ive`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.ive.html)). Now, this is replaced by a special-case ($\mu = 0.5$) where the analytic expression can be implemented in `pytorch` directly - see [this](https://github.com/elobre/bimm/commit/a33e308026f564fd03561396d7f8e61577f3e2fb) commit. Since no approximations are used, results produced with the new code are identical to the results presented in the paper. Only the timings mentioned in Section 5.1 will be different with the latest code release (4 min -> 15 sec). 